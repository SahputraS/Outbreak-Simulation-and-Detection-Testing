{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhO5Uvni+WBe600LXXwuNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SahputraS/Outbreak-Simulation-and-Detection-Testing/blob/main/Rio_Dengue_Risk_Classification_XGBOOST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdaK-oNBaht9"
      },
      "outputs": [],
      "source": [
        "# ---- base\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ---- viz (keep only what you use)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "import plotly.express as px\n",
        "\n",
        "# ---- sklearn / imbalanced-learn / xgboost\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV, TimeSeriesSplit, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import (\n",
        "    balanced_accuracy_score, make_scorer,\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    average_precision_score, roc_auc_score,\n",
        "    precision_recall_curve, confusion_matrix, classification_report,\n",
        ")\n",
        "\n",
        "from imblearn.pipeline import Pipeline           # must be imblearn's Pipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# ---- other libs you referenced\n",
        "from itertools import product\n",
        "from tqdm.auto import tqdm\n",
        "from statsmodels.tsa.stattools import ccf\n",
        "\n",
        "# ---- tensorflow (only if you actually use it below)\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(ibge, ey_start, ey_end):\n",
        "  url = \"https://info.dengue.mat.br/api/alertcity\"\n",
        "  geocode = ibge\n",
        "  disease = \"dengue\"\n",
        "  format = \"csv\"\n",
        "  ew_start = 1\n",
        "  ew_end = 53\n",
        "  ey_start = ey_start\n",
        "  ey_end = ey_end\n",
        "\n",
        "  params =(\n",
        "      \"&disease=\"\n",
        "      + f\"{disease}\"\n",
        "      + \"&geocode=\"\n",
        "      + f\"{geocode}\"\n",
        "      + \"&disease=\"\n",
        "      + f\"{disease}\"\n",
        "      + \"&format=\"\n",
        "      + f\"{format}\"\n",
        "      + \"&ew_start=\"\n",
        "      + f\"{ew_start}\"\n",
        "      + \"&ew_end=\"\n",
        "      + f\"{ew_end}\"\n",
        "      + \"&ey_start=\"\n",
        "      + f\"{ey_start}\"\n",
        "      + \"&ey_end=\"\n",
        "      + f\"{ey_end}\"\n",
        "  )\n",
        "\n",
        "  url_resp = \"?\".join([url, params])\n",
        "  dados = pd.read_csv(url_resp, index_col='SE')\n",
        "  return dados"
      ],
      "metadata": {
        "id": "xlXeROvmryI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # rio_neighbours_all = [3304557, 3302007, 3305554, 3303500,3301702, 3305109, 3300456, 3303203, 3302858, 3304144, 3302270, 3302858]\n",
        "rio_neighbours = [3304557, 3302007, 3305554, 3303500, 3301702, 3305109, 3303203, 3302858]"
      ],
      "metadata": {
        "id": "GVFTgNL-OrRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# geojson_url = \"https://raw.githubusercontent.com/tbrugz/geodata-br/master/geojson/geojs-100-mun.json\"\n",
        "\n",
        "# muni = gpd.read_file(geojson_url)\n",
        "# muni[\"id\"] = muni[\"id\"].astype(str)\n",
        "\n",
        "# rio = muni[muni[\"id\"].str.startswith(\"330\")].copy() # Municipality rio ibge starts with 330\n",
        "# ribge = [str(x).zfill(7) for x in rio_neighbours]\n",
        "\n",
        "# rpl = pd.DataFrame({\"ibge\": rio[\"id\"]})\n",
        "# rpl[\"selected\"] = \"other\"\n",
        "# rpl.loc[rpl[\"ibge\"].isin(ribge), \"selected\"] = \"neighbour\"\n",
        "# rpl.loc[rpl[\"ibge\"] == \"3304557\", \"selected\"] = \"rio_city\"\n",
        "\n",
        "# fig = px.choropleth(\n",
        "#     rpl,\n",
        "#     geojson=rio.__geo_interface__,\n",
        "#     locations=\"ibge\",\n",
        "#     featureidkey=\"properties.id\",\n",
        "#     color=\"selected\",\n",
        "#     color_discrete_map={\n",
        "#         \"other\":   \"lightgrey\",\n",
        "#         \"neighbour\": \"orange\",\n",
        "#         \"rio_city\": \"maroon\",\n",
        "#     }\n",
        "# )\n",
        "\n",
        "# fig.update_traces(marker_line_width=0.4, marker_line_color=\"black\")\n",
        "# fig.update_geos(fitbounds=\"locations\", visible=False)\n",
        "# fig.update_layout(margin=dict(l=0, r=0, t=0, b=0), coloraxis_showscale=False)\n",
        "\n",
        "# fig.show()\n"
      ],
      "metadata": {
        "id": "FDQqOHx7QrRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rj = []\n",
        "for i, ibge in enumerate(rio_neighbours):\n",
        "    data = get_data(ibge=ibge, ey_start=2010, ey_end=2025)\n",
        "    data['region'] = f\"r{i+1}\"\n",
        "    rj.append(data)\n",
        "\n",
        "all = pd.concat(rj, ignore_index=True)"
      ],
      "metadata": {
        "id": "qvbnNrCBMzpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all.columns"
      ],
      "metadata": {
        "id": "tnSdNkdqWHtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all.head(5)"
      ],
      "metadata": {
        "id": "N1KLGLn7M_FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ],
      "metadata": {
        "id": "YjF6HbTD9ij9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because I want to learn how to predict the nivel, and the source uses\n",
        "1. Weather\n",
        "2. Twitter\n",
        "3. Reported Cases"
      ],
      "metadata": {
        "id": "grXLuZNO29pC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all.columns"
      ],
      "metadata": {
        "id": "0fwY3Ynz2DYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['data_iniSE', 'nivel','p_inc100k', 'tweet', 'Rt', 'p_rt1', 'tempmin', 'umidmax', 'umidmed', 'umidmin', 'tempmed', 'tempmax', 'region']\n",
        "all_filt = all[cols].copy()"
      ],
      "metadata": {
        "id": "8njnoWf8HYpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_filt.isna().sum()"
      ],
      "metadata": {
        "id": "d35nlyfGGPqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ts(df, cols, date_col=\"data_iniSE\", region_col=\"region\", na_fill=0):\n",
        "\n",
        "    df = df.copy()\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
        "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
        "\n",
        "    df[cols] = df[cols].fillna(na_fill)\n",
        "\n",
        "    regions = df[region_col].unique()\n",
        "\n",
        "    for col in cols:\n",
        "        plt.figure(figsize=(17, 5))\n",
        "        for r in regions:\n",
        "            g = df[df[region_col] == r].sort_values(date_col)\n",
        "            plt.plot(g[date_col], g[col], marker='o', markersize=2, label=f\"Region {r}\")\n",
        "        plt.title(col)\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(col)\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "jkbvjxgiKTa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vars_to_plot = ['p_inc100k','tweet','tempmin','umidmax','umidmed','umidmin','tempmed','tempmax']\n",
        "plot_ts(all_filt, vars_to_plot, date_col=\"data_iniSE\", region_col=\"region\", na_fill=-5)"
      ],
      "metadata": {
        "id": "W--B6AldKYo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## From the graph above, tweet data seems to exists for only the rio, therefore I will not include tweets\n",
        "all_filt2 = all_filt.copy()\n",
        "all_filt2.drop(columns=['tweet'], inplace=True)\n",
        "all_filt2.head(5)"
      ],
      "metadata": {
        "id": "XLD8O2zEElAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Model 1\n",
        "Tomorrow is the same as today"
      ],
      "metadata": {
        "id": "I9ats9lbBucG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_b1 = all_filt2[['data_iniSE', 'nivel', 'region']].copy()\n",
        "data_b1['pred'] = data_b1['nivel'].shift(-1)\n",
        "data_b1 = data_b1[data_b1['data_iniSE'] != pd.to_datetime('2010-01-03')]\n",
        "data_b1 = data_b1.dropna()"
      ],
      "metadata": {
        "id": "ev5dyeeZB0DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_b1.head(5)"
      ],
      "metadata": {
        "id": "RExgwhXdVXxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(y_true, y_pred, region=None):\n",
        "    acc = round(accuracy_score(y_true, y_pred), 3)\n",
        "    precision = round(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0), 3)\n",
        "    recall = round(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0), 3)\n",
        "    f1 = round(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0), 3)\n",
        "\n",
        "    row = pd.DataFrame([{\n",
        "        \"region\": region,\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }])\n",
        "    return row"
      ],
      "metadata": {
        "id": "z7i0sDMLV4L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = data_b1['region'].unique().tolist()\n",
        "eval_baseline = pd.DataFrame()\n",
        "\n",
        "for _, r in enumerate(reg):\n",
        "    data = data_b1[data_b1['region']==r]\n",
        "    eval_baseline = pd.concat([eval_baseline, eval(data['nivel'], data['pred'], r)], ignore_index=True)"
      ],
      "metadata": {
        "id": "GgTxvovUXi8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_baseline"
      ],
      "metadata": {
        "id": "pq9AKS48Y86c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation For XGBOOST"
      ],
      "metadata": {
        "id": "bTtEXrKKXUcH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the data to make the brazilian dataset to be more general (based on only incidence and weathers info) by:\n",
        "- Making the pseudo Rt from the incidence cases (to be added)\n",
        "- Change the nivel (alarm level) into 0/1 where 0 is flat or going down and 1 is if there is an increase in level\n",
        "- Make the predictors are the average of some weeks"
      ],
      "metadata": {
        "id": "F-Gy9kpRkCns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_lags(df, cols, max_lag=4, include_lag0=False):\n",
        "\n",
        "    df = df.sort_values(['region','data_iniSE']).copy()\n",
        "    g = df.groupby('region', group_keys=False)\n",
        "    for c in cols:\n",
        "        if include_lag0:\n",
        "            df[f\"{c}_lag0\"] = g[c].shift(0)\n",
        "        for L in range(1, max_lag + 1):\n",
        "            df[f\"{c}_lag{L}\"] = g[c].shift(L)\n",
        "    return df"
      ],
      "metadata": {
        "id": "oJl1LudTeI9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep(df, h=1, train_frac=0.75, lag_cols=None, max_lag=4, include_lag0=False):\n",
        "\n",
        "    df['data_iniSE'] = pd.to_datetime(df['data_iniSE'])\n",
        "    df = df.sort_values(['region','data_iniSE'])\n",
        "\n",
        "    # 1) Target: up=1 vs last week, then shift FORWARD by horizon h\n",
        "    df['nivel_binary'] = df.groupby('region')['nivel'].diff().gt(0).astype(int).fillna(0)\n",
        "    df['nivel_binary'] = df.groupby('region')['nivel_binary'].shift(-h)\n",
        "\n",
        "    # 2) Drop unused raw cols if present\n",
        "    for c in ['Rt','p_rt1','nivel']:\n",
        "        if c in df.columns:\n",
        "            df = df.drop(columns=c)\n",
        "\n",
        "    # 3) Choose columns to lag (default: all numeric except id/date/target)\n",
        "    if lag_cols is None:\n",
        "        non_feat = {'data_iniSE','region','nivel_binary'}\n",
        "        lag_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in non_feat]\n",
        "\n",
        "    # 4) Add lags (past-only, no leakage)\n",
        "    df = add_lags(df, lag_cols, max_lag=max_lag, include_lag0=include_lag0)\n",
        "\n",
        "    # 5) Figure out which lag columns were added to enforce completeness\n",
        "    added_lag_cols = []\n",
        "    if include_lag0:\n",
        "        added_lag_cols += [f\"{c}_lag0\" for c in lag_cols]\n",
        "    added_lag_cols += [f\"{c}_lag{L}\" for c in lag_cols for L in range(1, max_lag+1)]\n",
        "\n",
        "    # 6) Per-region chronological split; drop rows missing target/lag features\n",
        "    regs = df['region'].unique()\n",
        "    tr_parts, te_parts = [], []\n",
        "    need_cols = ['nivel_binary'] + added_lag_cols\n",
        "\n",
        "    for r in regs:\n",
        "        gdf = df[df['region'] == r].copy()\n",
        "        n = len(gdf)\n",
        "        tr_end = int(np.floor(train_frac * n))\n",
        "        print(f\"region: {r}, total week: {n}, train until week: {tr_end}\")\n",
        "\n",
        "        g_tr = gdf.iloc[:tr_end].dropna(subset=need_cols)\n",
        "        g_te = gdf.iloc[tr_end:].dropna(subset=need_cols)\n",
        "\n",
        "        tr_parts.append(g_tr)\n",
        "        te_parts.append(g_te)\n",
        "\n",
        "    data_ml  = pd.concat(tr_parts, ignore_index=True).dropna()\n",
        "    data_test = pd.concat(te_parts, ignore_index=True).dropna()\n",
        "    return data_ml, data_test"
      ],
      "metadata": {
        "id": "P7UPSkjhwwvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_ml, data_test = prep(all_filt2)\n",
        "data_ml.head(5)"
      ],
      "metadata": {
        "id": "ay9_Sc3qJzi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xy split"
      ],
      "metadata": {
        "id": "WG93SqSbcNtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NON_FEAT = {\"data_iniSE\", \"region\", \"nivel_binary\"}\n",
        "\n",
        "def make_Xy(df):\n",
        "    cols = [c for c in df.columns if c not in NON_FEAT]\n",
        "    # cols = ['p_inc100k', 'tempmin', 'tempmin_lag1', 'p_inc100k_lag1']\n",
        "    X = df[cols]\n",
        "    y = df[\"nivel_binary\"].astype(int)\n",
        "    return X, y, cols"
      ],
      "metadata": {
        "id": "OcjsGK7jRSSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_raw, y_train, feat_cols = make_Xy(data_ml)\n",
        "X_test_raw = data_test.reindex(columns=feat_cols)\n",
        "y_test = data_test[\"nivel_binary\"].astype(int)"
      ],
      "metadata": {
        "id": "iQYVAjQWRY5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Imbalance - SMOT-EEN Sampling (Illustration Purposes)"
      ],
      "metadata": {
        "id": "JoJEjjpBRZ32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_pie (y, title=\"\"):\n",
        "    vc = pd.Series(y).value_counts().sort_index()\n",
        "    labels = [f\"{cls} ({cnt}, {cnt/len(y):.1%})\" for cls, cnt in vc.items()]\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.pie(vc.values, labels=labels,  colors=[\"darkblue\", \"darkred\"], startangle=90)\n",
        "    plt.title(title)\n",
        "    plt.axis(\"equal\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Qe8tc1y9RdL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pie(y_train, \"Train label distribution (before SMOTE)\")\n",
        "plot_pie(y_test,  \"Test label distribution\")"
      ],
      "metadata": {
        "id": "5uihYL4qRv-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b.\n",
        "\n",
        "EEN (Edited Nearest Neighbors) is a \"cleaning\" mechanism.\n",
        "\n",
        "By combining SMOTE with ENN, SMOTENN is able to generate synthetic samples that are more representative of the minority class and reduce the presence of noisy samples. This can lead to improved generalization performance of the model."
      ],
      "metadata": {
        "id": "op5s4GgFUtll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOTE-EEN Sampling\n",
        "smenn = SMOTEENN()\n",
        "X_train2, y_train2 = smenn.fit_resample (X_train_raw, y_train)"
      ],
      "metadata": {
        "id": "20dr77JtSxUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_pie(y_train2, \"Train label distribution (after SMOTE)\")"
      ],
      "metadata": {
        "id": "CCCghU0nW8un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGB Models"
      ],
      "metadata": {
        "id": "LRNvUMIjICw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "select_param_xgb = XGBClassifier(\n",
        "    objective=\"binary:logistic\", eval_metric=\"aucpr\",\n",
        "    n_estimators=300, max_depth=3, learning_rate=0.06,\n",
        "    subsample=0.9, colsample_bytree=0.9, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "HWf5OVmpIAHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fin_xgb = XGBClassifier(\n",
        "    objective=\"binary:logistic\", eval_metric=\"aucpr\",\n",
        "    tree_method=\"hist\", random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "gAfhUPSsJulC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make Pipeline For Gridsearch"
      ],
      "metadata": {
        "id": "f3of2OW9K_hI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline([\n",
        "    (\"scale\",    StandardScaler()),\n",
        "    (\"smoteenn\", SMOTEENN(smote=SMOTE(random_state=42), random_state=42)),\n",
        "    (\"select\",   SelectFromModel(select_param_xgb, threshold=\"median\")),\n",
        "    (\"xgb\",      fin_xgb),\n",
        "])"
      ],
      "metadata": {
        "id": "lJeP-zx9K7UL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gridsearch"
      ],
      "metadata": {
        "id": "LKZ12LQhLmI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    # \"smoteenn__sampling_strategy\": [0.5, 0.7, 1.0],\n",
        "    # \"smoteenn__smote__k_neighbors\": [3, 5],\n",
        "    \"select__threshold\": [\"median\", \"mean\"],\n",
        "    \"xgb__n_estimators\": [400, 800],\n",
        "    \"xgb__max_depth\": [3, 4, 5],\n",
        "    \"xgb__learning_rate\": [0.03, 0.06],\n",
        "    \"xgb__subsample\": [0.8, 1.0],\n",
        "    \"xgb__colsample_bytree\": [0.8, 1.0],\n",
        "}"
      ],
      "metadata": {
        "id": "mBbNc4bwLfE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# gs = GridSearchCV(\n",
        "#     estimator=pipe,\n",
        "#     param_grid=param_grid,\n",
        "#     #scoring via balanced acc due to imbalanced data\n",
        "#     scoring=make_scorer(balanced_accuracy_score),\n",
        "#     cv=cv,\n",
        "#     n_jobs=-1,\n",
        "#     refit=True,\n",
        "#     verbose=0,\n",
        "# )"
      ],
      "metadata": {
        "id": "MDIc6SHt8A4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gs.fit(X_train_raw, y_train)\n",
        "\n",
        "# print(\"Best CV balanced accuracy:\", gs.best_score_)\n",
        "# print(\"Best params:\", gs.best_params_)"
      ],
      "metadata": {
        "id": "JDxGTv1SNOZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model = gs.best_estimator_"
      ],
      "metadata": {
        "id": "ZpPTIDCvRDt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selector    = best_model.named_steps[\"select\"]\n",
        "\n",
        "# mask = selector.get_support()\n",
        "\n",
        "# if hasattr(X_train_raw, \"columns\"):\n",
        "#     all_cols = np.asarray(X_train_raw.columns)\n",
        "# else:\n",
        "#     all_cols = np.asarray(feature_cols)\n",
        "\n",
        "# selected_cols = all_cols[mask]\n",
        "# dropped_cols  = all_cols[~mask]\n",
        "\n",
        "# print(f\"{mask.sum()} features kept / {len(all_cols)} total\")\n",
        "# print(\"Selected features:\", selected_cols.tolist())"
      ],
      "metadata": {
        "id": "Cb_owZZyTy-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "horizon = 4\n",
        "best_mod_list = []\n",
        "\n",
        "for hh in range(horizon):\n",
        "\n",
        "  print(\"Process horizon\", hh+1)\n",
        "\n",
        "  ## Make dataset\n",
        "  data_ml, data_test = prep(all_filt2, h=hh+1)\n",
        "\n",
        "  ## Split Xy\n",
        "  X_train_raw, y_train, feat_cols = make_Xy(data_ml)\n",
        "  X_test_raw = data_test.reindex(columns=feat_cols)\n",
        "  y_test = data_test[\"nivel_binary\"].astype(int)\n",
        "\n",
        "  ## Do gridsearch per horizon\n",
        "  gs.fit(X_train_raw, y_train)\n",
        "\n",
        "  print(\"For horizon\", hh+1, \"best CV balanced accuracy:\", gs.best_score_)\n",
        "  print(\"For horizon\", hh+1, \"best params:\", gs.best_params_)\n",
        "\n",
        "  # Take best model\n",
        "  best_model = gs.best_estimator_\n",
        "  best_mod_list.append(best_model)\n",
        "\n",
        "  selector    = best_model.named_steps[\"select\"]\n",
        "\n",
        "  mask = selector.get_support()\n",
        "\n",
        "  if hasattr(X_train_raw, \"columns\"):\n",
        "      all_cols = np.asarray(X_train_raw.columns)\n",
        "  else:\n",
        "      all_cols = np.asarray(feature_cols)\n",
        "\n",
        "  selected_cols = all_cols[mask]\n",
        "  dropped_cols  = all_cols[~mask]\n",
        "\n",
        "  print(f\"For horizon {hh+1}, {mask.sum()} features kept / {len(all_cols)} total\")\n",
        "  print(\"For horizon\", hh+1, \"selected features:\", selected_cols.tolist())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NSUtXRUPeTz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best models to a file\n",
        "with open(\"best_models.txt\", \"w\") as f:\n",
        "    for i, model in enumerate(best_mod_list):\n",
        "        f.write(f\"--- Horizon {i+1} ---\\n\")\n",
        "        f.write(str(model))\n",
        "        f.write(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "K19MjOwxzGNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "31U1AXenzZgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c657eb9c"
      },
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "shutil.copy(\"best_models.txt\", \"/content/drive/MyDrive/best_models.txt\")\n",
        "print(\"best_models.txt saved to Google Drive.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93838fae"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "YoJPYjRtY3nC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  ## Make dataset\n",
        "  data_ml, data_test = prep(all_filt2, h=hh+1)\n",
        "\n",
        "  ## Split Xy\n",
        "  X_train_raw, y_train, feat_cols = make_Xy(data_ml)\n",
        "  X_test_raw = data_test.reindex(columns=feat_cols)\n",
        "  y_test = data_test[\"nivel_binary\"].astype(int)\n"
      ],
      "metadata": {
        "id": "sSnpyz_M0FF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_df = pd.DataFrame()\n",
        "\n",
        "i = 0\n",
        "for best_model in best_mod_list:\n",
        "\n",
        "  print(\"Process horizon\", i+1)\n",
        "\n",
        "  ## Make dataset\n",
        "  data_ml, data_test = prep(all_filt2, h=i+1) #fix later to streamline better\n",
        "  i +=1\n",
        "\n",
        "  ## Split Xy\n",
        "  X_train_raw, y_train, feat_cols = make_Xy(data_ml)\n",
        "  X_test_raw = data_test.reindex(columns=feat_cols)\n",
        "  y_test = data_test[\"nivel_binary\"].astype(int)\n",
        "\n",
        "  y_pred = best_model.predict(X_test_raw)\n",
        "\n",
        "  # Calculate evaluation metrics\n",
        "  accuracy = balanced_accuracy_score(y_test, y_pred) #accuracy_score(y_test, y_pred)\n",
        "  f1 = f1_score(y_test, y_pred)\n",
        "  recall = recall_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "\n",
        "  print(\"XGBoost Balanced Accuracy:\", round(accuracy,2))\n",
        "  print(\"F1 Score:\", round(f1,2))\n",
        "  print(\"Recall:\", round(recall,2))\n",
        "  print(\"Precision:\", round(precision,2))\n",
        "\n",
        "  # PLot Heeatmap\n",
        "  cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "  plt.figure(figsize=(8, 6))\n",
        "  sns.set_theme(style=\"whitegrid\")\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "  plt.title(f\"Horizon {i}\")\n",
        "  plt.xlabel('Predicted')\n",
        "  plt.ylabel('Actual')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "_4HOyAkcy7b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hwEkOmAT1tb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}